{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628ebe8a-a85a-40af-b896-e69a95c3aae9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **LINGUISTIC ANALYSIS OF THE COOKING WEBSITE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf716b33-0353-4242-9a6a-4d622e435888",
   "metadata": {},
   "source": [
    "In this interactive essay, I will go through collecting data from a cooking website by using web scraping. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c48c070-d554-46f6-b1be-16f7126a4b62",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728aeaa-25eb-4cc8-ade6-8f09263bf89e",
   "metadata": {},
   "source": [
    "For the linguistic analysis I have chosen the page devoted to cooking. Below I present the data collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e48aa5-2ba7-4b00-8500-f4e50e59aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4390e4-5a1e-47cb-8684-ab7ec5a32633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "page = requests.get(\"https://www.bbcgoodfood.com/\")\n",
    "print(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c352289-8e9a-4b32-83ec-1b6c0bf8f0d4",
   "metadata": {},
   "source": [
    "At this point, we have our new variable called \"page\" to which our page value has been assigned. However, the output data is not readable at this stage but we can improve it using Beautiful Soup library. \n",
    "Let's move to subsection about data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888be5f2-e4ec-46a6-9c30-356b9c806591",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d155efc8-7479-412c-acae-251a3b66f818",
   "metadata": {},
   "source": [
    "Using the Beautiful soup library, we split the text into paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f8359-325c-4932-a590-c18f89e67c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f25b3-74a8-465d-a2f9-1dc8f27fc0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = soup.find_all('p')\n",
    "print(paras)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e194eae-781b-4a21-9c04-56b7b8b64502",
   "metadata": {},
   "source": [
    "Now we have created a new variable \"paragraph\" to which all the scratched paragraphs have been assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68cddb-28c1-4172-ba99-9d5f7ab33e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_text = []\n",
    "for para in paras:\n",
    "    processed_para = para.get_text()\n",
    "    processed_para = processed_para.strip()\n",
    "    if len(processed_para) > 1:\n",
    "        only_text.append(processed_para)\n",
    "print(only_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3e550-6679-4f59-8dd1-fd6853013b4e",
   "metadata": {},
   "source": [
    "By using an empty list and appending it with a simple for loop, we made the text appear more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3774911e-a4f0-4369-8243-beed7c3ad7c8",
   "metadata": {},
   "source": [
    "### Data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e24bb1a-ce96-42b2-8d9f-4fd9a26fd36b",
   "metadata": {},
   "source": [
    "#### 1.1 Part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee7df6-605e-4de2-b8a2-d2710ffb0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = []\n",
    "for sentence in only_text:\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    for item in tagged:\n",
    "        tuples.append(item)\n",
    "print(tuples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5dd9dc-8c5d-43ee-a3f8-2ede7330fa12",
   "metadata": {},
   "source": [
    "At this stage by using the word_tokenize nltk function we create an empty tuple in which we insert tokenized words. Each tuple displayed contains a token and its corresponding part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d6f71-f28e-436f-a703-cda89fa50567",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_dict = {}\n",
    "for item in tuples:\n",
    "    if item[1] not in counter_dict:\n",
    "        counter_dict[item[1]] = 1\n",
    "    else:\n",
    "        counter_dict[item[1]] += 1\n",
    "\n",
    "print(counter_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ce694-ab3c-4ebb-b212-79c389ce2dc0",
   "metadata": {},
   "source": [
    "With this code we can easily count the occurrences of a given part of speech. \n",
    "In addition we can visualize the data as a graph using a **matplotlib**. The result is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b8e5b-3a11-48ba-be35-8c60267507ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mat\n",
    "mat.figure(figsize=(20,10))\n",
    "mat.bar(counter_dict.keys(), counter_dict.values(), color='green')\n",
    "mat.xlabel('Part of speech', size=20)\n",
    "mat.ylabel('Data visualization: number of appearances', size=15)\n",
    "mat.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7f986-074e-4f40-a611-05cafe3480e2",
   "metadata": {},
   "source": [
    "As we can see common nouns (NN) most often appear in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3974aa-9ca4-4c28-8327-6a032fb397e3",
   "metadata": {},
   "source": [
    "#### 1.2 Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f24d5-aee8-42f7-a358-496a783fbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = '''!()-[]{};:'\"\\,<>.?@#$%^&*_~<=>+/(|){`\\}/'''\n",
    "punctuation_to_remove = soup.prettify()\n",
    "no_punctuation = \" \"\n",
    "for char in punctuation_to_remove:\n",
    "    no_punctuation = no_punctuation + char\n",
    "print(no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47177907-24e0-4353-9eb3-025fb6c828a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = []\n",
    "for sentence in only_text:\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8d14c-963a-48c3-8609-cb4801e754db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8dd702-247f-4044-b51d-39b3d4ef6290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
